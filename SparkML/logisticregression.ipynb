{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Spark\n",
    "\n",
    "The intent of this blog is to demonstrate binary classification in pySpark. The various steps involved in developing a classification model in pySpark are as follows:\n",
    "\n",
    "1) Initialize a Spark session\n",
    "\n",
    "2) Download and read the the dataset\n",
    "\n",
    "3) Developing initial understanding about the data\n",
    "\n",
    "4) Handling missing values\n",
    "\n",
    "5) Scalerizing the features\n",
    "\n",
    "6) Train test split\n",
    "\n",
    "7) Imbalance handling\n",
    "\n",
    "8) Feature selection\n",
    "\n",
    "9) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"diabeties\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and read the dataset\n",
    "\n",
    "For the purpose of demonstration I am using a dataset from using data from Pima Indians Diabetes Database. The dataset can be easily downloaded from this link https://www.kaggle.com/abhikaggle8/pima-diabetes-classification/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets us begin by reading the \"diabetes.csv\" and create a Spark dataframe named 'raw_data'\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|DiabetesPedigreeFunction|\n",
      "+------------------------+\n",
      "|                   0.627|\n",
      "|                   0.351|\n",
      "|                   0.672|\n",
      "|                   0.167|\n",
      "|                   2.288|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('DiabetesPedigreeFunction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, the data contains 9 columns. Each of these are described as follows:\n",
    "\n",
    "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "Columns\n",
    "\n",
    "1) Pregnancies: Number of times pregnant (numeric)\n",
    "\n",
    "2) Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (numeric)\n",
    "\n",
    "3) Blood Pressure: Diastolic blood pressure (mm Hg) (numeric)\n",
    "\n",
    "4) Skin Thickness: Triceps skin fold thickness (mm) (numeric)\n",
    "\n",
    "5) Insulin: 2-Hour serum insulin (mu U/ml) (numeric)\n",
    "\n",
    "6) BMI: Body mass index (weight in kg/(height in m)^2) (numeric)\n",
    "\n",
    "7) Diabetes Pedigree Function: Diabetes pedigree function (numeric)\n",
    "\n",
    "8) Age: Age of the person (years)\n",
    "\n",
    "9) Outcome: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ask is to buid a machine learning model to accurately predict whether or not the patients in the dataset have diabetes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|Summary|       Pregnancies|          Glucose|     BloodPressure|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|               768|              768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|\n",
      "|    min|                 0|                0|                 0|\n",
      "|    max|                17|              199|               122|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"Pregnancies\",\"Glucose\",\"BloodPressure\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|Summary|     SkinThickness|           Insulin|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               768|               768|\n",
      "|   mean|20.536458333333332| 79.79947916666667|\n",
      "| stddev|15.952217567727642|115.24400235133803|\n",
      "|    min|                 0|                 0|\n",
      "|    max|                99|               846|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"SkinThickness\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------------+------------------+\n",
      "|Summary|               BMI|DiabetesPedigreeFunction|               Age|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "|  count|               768|                     768|               768|\n",
      "|   mean|31.992578124999977|      0.4718763020833327|33.240885416666664|\n",
      "| stddev| 7.884160320375441|       0.331328595012775|11.760231540678689|\n",
      "|    min|               0.0|                   0.078|                21|\n",
      "|    max|              67.1|                    2.42|                81|\n",
      "+-------+------------------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Summary\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above tables, it is observed that the minimum value for the fields such as \"Pregnancies\", \"glucose\", \"blood pressure\", \"skin thickness\",\"insulin\" and \"BMI\" are zero (0) which seems impractical to me (except \"Pregnancies\"). \n",
    "\n",
    "Therefore, let us replace all the zeros in the abaove mentioned fields (except \"Pregnancies\") with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "raw_data=raw_data.withColumn(\"Glucose\",when(raw_data.Glucose==0,np.nan).otherwise(raw_data.Glucose))\n",
    "raw_data=raw_data.withColumn(\"BloodPressure\",when(raw_data.BloodPressure==0,np.nan).otherwise(raw_data.BloodPressure))\n",
    "raw_data=raw_data.withColumn(\"SkinThickness\",when(raw_data.SkinThickness==0,np.nan).otherwise(raw_data.SkinThickness))\n",
    "raw_data=raw_data.withColumn(\"BMI\",when(raw_data.BMI==0,np.nan).otherwise(raw_data.BMI))\n",
    "raw_data=raw_data.withColumn(\"Insulin\",when(raw_data.Insulin==0,np.nan).otherwise(raw_data.Insulin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------------+----+\n",
      "|Insulin|Glucose|BloodPressure|SkinThickness| BMI|\n",
      "+-------+-------+-------------+-------------+----+\n",
      "|    NaN|  148.0|         72.0|         35.0|33.6|\n",
      "|    NaN|   85.0|         66.0|         29.0|26.6|\n",
      "|    NaN|  183.0|         64.0|          NaN|23.3|\n",
      "|   94.0|   89.0|         66.0|         23.0|28.1|\n",
      "|  168.0|  137.0|         40.0|         35.0|43.1|\n",
      "|    NaN|  116.0|         74.0|          NaN|25.6|\n",
      "|   88.0|   78.0|         50.0|         32.0|31.0|\n",
      "|    NaN|  115.0|          NaN|          NaN|35.3|\n",
      "|  543.0|  197.0|         70.0|         45.0|30.5|\n",
      "|    NaN|  125.0|         96.0|          NaN| NaN|\n",
      "|    NaN|  110.0|         92.0|          NaN|37.6|\n",
      "|    NaN|  168.0|         74.0|          NaN|38.0|\n",
      "|    NaN|  139.0|         80.0|          NaN|27.1|\n",
      "|  846.0|  189.0|         60.0|         23.0|30.1|\n",
      "|  175.0|  166.0|         72.0|         19.0|25.8|\n",
      "|    NaN|  100.0|          NaN|          NaN|30.0|\n",
      "|  230.0|  118.0|         84.0|         47.0|45.8|\n",
      "|    NaN|  107.0|         74.0|          NaN|29.6|\n",
      "|   83.0|  103.0|         30.0|         38.0|43.3|\n",
      "|   96.0|  115.0|         70.0|         30.0|34.6|\n",
      "+-------+-------+-------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_data.select(\"Insulin\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have replaced all \"0\" with NaN. Now, we can simply impute the NaN by calling an imputer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+------------------+-----------------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|     SkinThickness|          Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+------------------+-----------------+----+------------------------+---+-------+\n",
      "|          6|  148.0|         72.0|              35.0|155.5482233502538|33.6|                   0.627| 50|      1|\n",
      "|          1|   85.0|         66.0|              29.0|155.5482233502538|26.6|                   0.351| 31|      0|\n",
      "|          8|  183.0|         64.0|29.153419593345657|155.5482233502538|23.3|                   0.672| 32|      1|\n",
      "|          1|   89.0|         66.0|              23.0|             94.0|28.1|                   0.167| 21|      0|\n",
      "|          0|  137.0|         40.0|              35.0|            168.0|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+------------------+-----------------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"],outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"])\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, If we see the \"Pregnancies\" column in raw_data it can be seen that the maximum count goes upto 17, which is quit unbelievable. This may be an outlier, but we will discuss on outlier detection and removal in some other time.\n",
    "\n",
    "Now, let us combine all the features in one single feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|features                                                           |\n",
      "+-------------------------------------------------------------------+\n",
      "|[148.0,72.0,35.0,33.6,155.5482233502538]                           |\n",
      "|[85.0,66.0,29.0,26.6,155.5482233502538]                            |\n",
      "|[183.0,64.0,29.153419593345657,23.3,155.5482233502538]             |\n",
      "|[89.0,66.0,23.0,28.1,94.0]                                         |\n",
      "|[137.0,40.0,35.0,43.1,168.0]                                       |\n",
      "|[116.0,74.0,29.153419593345657,25.6,155.5482233502538]             |\n",
      "|[78.0,50.0,32.0,31.0,88.0]                                         |\n",
      "|[115.0,72.40518417462484,29.153419593345657,35.3,155.5482233502538]|\n",
      "|[197.0,70.0,45.0,30.5,543.0]                                       |\n",
      "|[125.0,96.0,29.153419593345657,32.45746367239099,155.5482233502538]|\n",
      "|[110.0,92.0,29.153419593345657,37.6,155.5482233502538]             |\n",
      "|[168.0,74.0,29.153419593345657,38.0,155.5482233502538]             |\n",
      "|[139.0,80.0,29.153419593345657,27.1,155.5482233502538]             |\n",
      "|[189.0,60.0,23.0,30.1,846.0]                                       |\n",
      "|[166.0,72.0,19.0,25.8,175.0]                                       |\n",
      "|[100.0,72.40518417462484,29.153419593345657,30.0,155.5482233502538]|\n",
      "|[118.0,84.0,47.0,45.8,230.0]                                       |\n",
      "|[107.0,74.0,29.153419593345657,29.6,155.5482233502538]             |\n",
      "|[103.0,30.0,38.0,43.3,83.0]                                        |\n",
      "|[115.0,70.0,30.0,34.6,96.0]                                        |\n",
      "+-------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"],outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "raw_data=assembler.transform(raw_data)\n",
    "raw_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have created a feature vector. Now let us use StandardScaler to scalerize the newly created \"feature\" column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[148.0,72.0,35.0,...|[4.86267080568854...|\n",
      "|[85.0,66.0,29.0,2...|[2.79275012488869...|\n",
      "|[183.0,64.0,29.15...|[6.01262673946623...|\n",
      "|[89.0,66.0,23.0,2...|[2.92417366017756...|\n",
      "|[137.0,40.0,35.0,...|[4.50125608364412...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train, test split\n",
    "\n",
    "Now that the preprocessing of the data is complete. Let us split the dataset in training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether their is imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ones are 206\n",
      "Percentage of ones are 34.2762063228\n"
     ]
    }
   ],
   "source": [
    "dataset_size=float(train.select(\"Outcome\").count())\n",
    "numPositives=train.select(\"Outcome\").where('Outcome == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the percentage of ones in the dataset is just 34.27 % surely their is imbalance in the dataset. Thankfully, in the case of logistic regression we have a technique called \"Class Weighing\". I reccomend reading https://stackoverflow.com/questions/33372838/dealing-with-unbalanced-datasets-in-spark-mllib for the purpose of understanding.\n",
    "\n",
    "In our dataset (train) we have 34.27 % positives and 65.73 % negatives. Since negatives are in a majority. Therefore,logistic loss objective function should treat the positive class (Outcome == 1) with higher weight. For this purpose we calculate the BalancingRatio as follows:\n",
    "\n",
    "BalancingRatio= numNegatives/dataset_size\n",
    "\n",
    "Then against every Outcome == 1, we put BalancingRatio in column \"classWeights\", and  against every Outcome == 0, we put 1-BalancingRatio in column  \"classWeights\" \n",
    "\n",
    "In this way, we assign higher weightage to the minority class (i.e. positive class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657237936772\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print(BalancingRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       classWeights|\n",
      "+-------------------+\n",
      "|0.34276206322795344|\n",
      "|0.34276206322795344|\n",
      "|0.34276206322795344|\n",
      "|0.34276206322795344|\n",
      "|0.34276206322795344|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=train.withColumn(\"classWeights\", when(train.Outcome == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|Aspect                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|[2.562758938133151,7.274924068899646,3.29885013976358,5.367154589366346,0.47047140468429116]  |\n",
      "|[2.8256060087109103,5.621532235058818,3.640110499049468,5.207158111092553,1.8295247783934943] |\n",
      "|[2.9898854278220095,5.621532235058818,3.640110499049468,5.8035086210221465,2.4699748745925287]|\n",
      "|[3.1213089631108892,5.290853868290652,4.436384670716539,6.487129937282901,1.2349874372962644] |\n",
      "|[3.1213089631108892,6.6135673353633155,5.118905389288314,5.30897405181224,1.0820842307738696] |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using chisquareSelector\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "# css=ChiSqSelector().setFeaturesCol('Scaled_features').setLabelCol(\"Outcome\").setOutputCol(\"Aspect\")\n",
    "css = ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='Outcome',fpr=0.05)\n",
    "train=css.fit(train).transform(train)\n",
    "test=css.fit(test).transform(test)\n",
    "test.select(\"Aspect\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      1|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       1.0|\n",
      "|      0|       1.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = LogisticRegression().setWeightCol(\"classWeights\").setLabelCol(\"Outcome\").setFeaturesCol(\"Aspect\")\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Aspect\",weightCol=\"classWeights\",maxIter=10)\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .addGrid(lr.fitIntercept, [False, True])\\\n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "#     .build()\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The BinaryClassificationEvaluator uses areaUnderROC as the default metric. As o fnow we will continue with the same\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+\n",
      "|Outcome|       rawPrediction|prediction|         probability|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "|      0|[1.65033811818014...|       0.0|[0.83893674282499...|\n",
      "|      0|[1.48378390504867...|       0.0|[0.81514343702859...|\n",
      "|      0|[1.13055374802707...|       0.0|[0.75594107685349...|\n",
      "|      0|[0.21956570663058...|       0.0|[0.55467196242667...|\n",
      "|      0|[0.81105994041838...|       0.0|[0.69233532517613...|\n",
      "+-------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Outcome\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set is 0.81013887182\n",
      "The area under ROC for test set is 0.817511520737\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "\n",
    "To this point we have developed a classification model using logistic regression. However, the working of logistic regression depends upon the on a number of parameters. As of now we have worked with only the default parameters. Now, let s try to tune the hyperparameters and see whether it make any difference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you are unsure which parameters to tune pls use \"print(lr.explainParams())\" to get the list of parameters available for tuning  \n",
    "#print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of tunable parameters in LR\n",
    "\n",
    "1) aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
    "\n",
    "2) elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
    "\n",
    "3) family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
    "\n",
    "4) featuresCol: features column name. (default: features, current: Aspect)\n",
    "\n",
    "5) fitIntercept: whether to fit an intercept term. (default: True)\n",
    "\n",
    "6) labelCol: label column name. (default: label, current: Outcome)\n",
    "\n",
    "7) maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
    "\n",
    "8) predictionCol: prediction column name. (default: prediction)\n",
    "\n",
    "9) probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
    "\n",
    "10) rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
    "\n",
    "11) regParam: regularization parameter (>= 0). (default: 0.0)\n",
    "\n",
    "12) standardization: whether to standardize the training features before fitting the model. (default: True)\n",
    "\n",
    "13) threshold: Threshold in binary classification prediction, in range [0, 1].\n",
    "\n",
    "14) If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
    "\n",
    "15) thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
    "\n",
    "16) tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
    "\n",
    "17) weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (current: classWeights)\n",
    "\n",
    "\n",
    "Now let us tune some of these parameters and observe their effect on the performance of the algorithm.\n",
    "\n",
    "For the purpose of hyperparameter tuning we will consider the following parameters:\n",
    "\n",
    "1) aggregationDepth [2, 5, 10]\n",
    "\n",
    "2) elasticNetParam [0.0, 0.5, 1.0]\n",
    "\n",
    "3) fitIntercept [True / False]\n",
    "\n",
    "4) maxIter [10, 100, 1000]\n",
    "\n",
    "5) regParam [0.01, 0.5, 2.0]\n",
    "\n",
    "frist off all let us define a parameter grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.aggregationDepth,[2,5,10])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.fitIntercept,[False, True])\\\n",
    "    .addGrid(lr.maxIter,[10, 100, 1000])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_train=cvModel.transform(train)\n",
    "predict_test=cvModel.transform(test)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters and K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we have not seen any significant improvement in the performance metric after tuning the hyperparameters. Looks like, the default parameters have worked well for this problem. However, hyperparameter tuning is an important aspect while solving problems with ML and must not be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work:\n",
    "\n",
    "Few important things still remain, which I have not covered in this blog:\n",
    "    \n",
    "1) Outlier detection\n",
    "\n",
    "2) Imbalance handling \n",
    "\n",
    "The class weighing thechnique which we have used in this work is, currently, suitable only for logisticregression. However, in the case of other algorithms Random Forest, Naive Bayes, Support Vector Machine we may need to use techniques such as Synthetic Minority Oversampling Technique (SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/ml-tuning.html\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html \n",
    "\n",
    "https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
