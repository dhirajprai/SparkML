{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essentials of Feature Engineering in pyspark - Part I\n",
    "\n",
    "Data preprocessing in Spark\n",
    "\n",
    "The most commanly used data preprocessing techniques in Spark approaches are as follows \n",
    "  \n",
    "  1) VectorAssembler\n",
    "\n",
    "  2)Bucketing\n",
    "\n",
    "3)Scaling and normalization\n",
    "\n",
    " a) StandardScaler\n",
    "\n",
    " b) MinMAxScaler\n",
    "\n",
    " c) MaxAbsScaler\n",
    "\n",
    " d) Elementwise Product\n",
    "\n",
    " e) Normalizer\n",
    "\n",
    "4) Working with categorical features\n",
    "\n",
    "a) StringIndexer\n",
    "\n",
    "b) Converting Indexed values back to text\n",
    "\n",
    "c) Indexing in vectors\n",
    "\n",
    "d) One-hot encoding\n",
    "\n",
    "5) Text data transformers\n",
    "\n",
    "a) tokenizing text\n",
    "\n",
    "b) Removing common words\n",
    "\n",
    "c) Creating word combinations\n",
    "\n",
    "d) Converting words into numerical representations\n",
    "\n",
    "e) Tf-Idf\n",
    "\n",
    "f) Word2Vec\n",
    "\n",
    "6) Feature Manipulation\n",
    "\n",
    "7) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset\n",
    "For the purpose of this demonstration we will be using three different datasets\n",
    "\n",
    "1) retail-data/by-day\n",
    "\n",
    "2) simple-ml-integers\n",
    "\n",
    "3) simple-ml\n",
    "\n",
    "4) simple-ml-scaling\n",
    "\n",
    "The datasets can be downloaded from this link https://github.com/databricks/Spark-The-Definitive-Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Lets us begin by reading the \"retail-data/by-day\" which is in .csv format\n",
    "# sales = spark.read.format(\"csv\") \\ # here we space the format of the file we intend to read\n",
    "#         .option(\"header\",\"true\") \\ # setting \"header\" as true will consider the first row as the header of the Dataframe\n",
    "#         .option(\"inferSchema\", \"true\") \\ # Spark has its own mechanism to infer the schema which I will leverage at this poit of time\n",
    "#         .load(\"/data/retail-data/by-day/*.csv\") \\ # here we specify the path to our csv file(s)\n",
    "#         .coalesce(5)\\\n",
    "#         .where(\"Description IS NOT NULL\") # We intend to take only those rows in the which the value in the description column is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets us begin by reading the \"retail-data/by-day\" which is in .csv format and save it into a Spark dataframe named 'sales'\n",
    "sales = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"data/retail-data/by-day/*.csv\").coalesce(5).where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets us read the parquet files in \"simple-ml-integers\" and make a Spark dataframe named 'fakeIntDF'\n",
    "fakeIntDF=spark.read.parquet(\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml-integers\")\n",
    "# Lets us read the parquet files in \"simple-ml\" and make a Spark dataframe named 'simpleDF'\n",
    "simpleDF=spark.read.json(r\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml\")\n",
    "# Lets us read the parquet files in \"simple-ml-scaling\" and make a Spark dataframe named 'scaleDF'\n",
    "scaleDF=spark.read.parquet(r\"/home/spark/DhirajR/Spark/feature_engineering/data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector assembler\n",
    "\n",
    "The vector assembler is basically use to concatenate all the features into a single vector which can be further passed to the estimator or ML algorithm. In order to demo the 'Vector Assembler' we will use the 'fakeIntDF' which we had created in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us see what kind of data do we have in 'fakeIntDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   7|   8|   9|\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF.cache()\n",
    "fakeIntDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|int1|int2|int3|     features|\n",
      "+----+----+----+-------------+\n",
      "|   7|   8|   9|[7.0,8.0,9.0]|\n",
      "|   1|   2|   3|[1.0,2.0,3.0]|\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|\n",
      "+----+----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Once the Vector assembler is imported we are required to create the object of the same. Here I will create an object anmed va\n",
    "# The above result shows that we have three features in 'FakeIntDF' i.e. int1, int2, int3. Let us create the object va so as to combine the three features into a single column named features\n",
    "assembler = VectorAssembler(inputCols=[\"int1\", \"int2\", \"int3\"],outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "assembler.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing\n",
    "Bucketing is a most straight forward approach for fro converting the contonuous variables into categorical variable let us understand this with an example straight away\n",
    "\n",
    "In pyspark the task of bucketing can be easily accomplished using the Bucketizer class.\n",
    "\n",
    "Firstly, We shall accomplish the noop task of creating bucket borders. Let us define a list\n",
    "bucketBorders =[-1.0, 5.0,10.0,250.0,600.0]\n",
    "\n",
    "Next, let us create a object of the Bucketizer class. Then we will apply the transform method to our target Dataframe \"dataframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us create a sample dataframe for demo purpose\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------+\n",
      "|features|Bucketizer_433da22e7cb656b7f278__output|\n",
      "+--------+---------------------------------------+\n",
      "|  -999.9|                                    0.0|\n",
      "|    -0.5|                                    1.0|\n",
      "|    -0.3|                                    1.0|\n",
      "|     0.0|                                    2.0|\n",
      "|     0.2|                                    2.0|\n",
      "|   999.9|                                    3.0|\n",
      "+--------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders=[-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "bucketer=Bucketizer().setSplits(bucketBorders).setInputCol(\"features\")\n",
    "bucketer.transform(dataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and normalization\n",
    "\n",
    "Scaling and normalization is another common task that we come across while handling continuous varaibles. It is not always imperative to scale and normalize the features. However, it is highly recommended to scale and normalize the features before applying an ML algorithm in order to avert the risk of an algorithm being insensitive to a certain features.\n",
    "\n",
    "Spark ML provides us with a class \"StandardScaler\" for easy scaling and normaization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|     Scaled_features|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|[3.58568582800318...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "# Let us create an object of StandardScaler class\n",
    "Scalerizer=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "Scalerizer.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler\n",
    "\n",
    "The StandardScaler standardizes the features with a zero mean and standard deviation of 1. Sometimes, we encounter situations where we need to scale values within a given range (i.e. max and min). For such task Spark ML provdies a MinMaxScaler.\n",
    "\n",
    "The StandardScaler and MinMaxScaler share the common soul, the only difference is that we can provide the minimum value and maximum values within which we wish to scale the features.\n",
    "\n",
    "For the sake of illustration, let us scale the features in the range 5 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------------+\n",
      "| id|      features|MinMax_Scaled_features|\n",
      "+---+--------------+----------------------+\n",
      "|  0|[1.0,0.1,-1.0]|         [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|         [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|         [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|         [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|      [10.0,10.0,10.0]|\n",
      "+---+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# Let us create an object of MinMaxScaler class\n",
    "MinMaxScalerizer=MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\").setOutputCol(\"MinMax_Scaled_features\")\n",
    "MinMaxScalerizer.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinAbsScaler\n",
    "\n",
    "Sometimes we need to scalerize features between -1 to 1. The MinAbsScaler does exactly this by dividing the features by the maximum absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------------+\n",
      "| id|      features|MinAbs_Scaled_features|\n",
      "+---+--------------+----------------------+\n",
      "|  0|[1.0,0.1,-1.0]|  [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|  [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|  [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|  [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|         [1.0,1.0,1.0]|\n",
      "+---+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "# Let us create an object of MinAbsScaler class\n",
    "MinAbsScalerizer=MaxAbsScaler().setInputCol(\"features\").setOutputCol(\"MinAbs_Scaled_features\")\n",
    "MinAbsScalerizer.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElementwiseProduct\n",
    "\n",
    "What differentiates ElementwiseProduct from the previously mentioned scalizers is the fact that, in ElementwiseProduct the features are scaled based on a multiplying factor. \n",
    "\n",
    "The below mentioned code snippet will transform the feature#1 --> 10 times, feature#2 --> 0.1 times and feature#3 --> -1 times \n",
    "\n",
    "For example --> the features [10, 20, 30] if scaled by [10, 0.1, -1] will become [100, 2.0, -30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|  ElementWiseProduct|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[10.0,0.010000000...|\n",
      "|  1| [2.0,1.1,1.0]|[20.0,0.110000000...|\n",
      "|  0|[1.0,0.1,-1.0]|[10.0,0.010000000...|\n",
      "|  1| [2.0,1.1,1.0]|[20.0,0.110000000...|\n",
      "|  1|[3.0,10.1,3.0]|    [30.0,1.01,-3.0]|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Let us define a scaling vector \n",
    "\n",
    "ScalebyVector=Vectors.dense([10,0.1,-1])\n",
    "\n",
    "# Let us create an object of the class Elementwise product\n",
    "ScalingUp=ElementwiseProduct().setScalingVec(ScalebyVector).setInputCol(\"features\").setOutputCol(\"ElementWiseProduct\")\n",
    "# Let us transform\n",
    "ScalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizer\n",
    "\n",
    "The normalizer allows the user to calculate distance between features. The most commonly used distance metircs are \"Manhattan distance\" and the \"Euclidean distance\". The Normalizer takes a parameter \"p\" from the user which represents the power norm.\n",
    "\n",
    "For example, Manhatan norm (Mahnatan distance) p = 1; Euclidean norm (Euclidean distance) p = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|  Manhattan Distance|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|[0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|[0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|[0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|[0.18633540372670...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "# Let us create an object of the class Normalizer product\n",
    "ManhattanDistance=Normalizer().setP(1).setInputCol(\"features\").setOutputCol(\"Manhattan Distance\")\n",
    "EuclideanDistance=Normalizer().setP(2).setInputCol(\"features\").setOutputCol(\"Euclidean Distance\")\n",
    "# Let us transform\n",
    "ManhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|  Euclidean Distance|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[0.70534561585859...|\n",
      "|  1| [2.0,1.1,1.0]|[0.80257235390512...|\n",
      "|  0|[1.0,0.1,-1.0]|[0.70534561585859...|\n",
      "|  1| [2.0,1.1,1.0]|[0.80257235390512...|\n",
      "|  1|[3.0,10.1,3.0]|[0.27384986857909...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EuclideanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Categorical features\n",
    "\n",
    "Most of the ML algorithms require converting categorical features into numerical ones. \n",
    "\n",
    "Sparks StringIndexer maps strings nto different numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red|good|    45| 38.97187133755819|\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply string indexer to a categorical variable named \"lab\" in \"simpleDF\" DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+------------+\n",
      "|color| lab|value1|            value2|LabelIndexed|\n",
      "+-----+----+------+------------------+------------+\n",
      "|green|good|     1|14.386294994851129|         1.0|\n",
      "| blue| bad|     8|14.386294994851129|         0.0|\n",
      "| blue| bad|    12|14.386294994851129|         0.0|\n",
      "|green|good|    15| 38.97187133755819|         1.0|\n",
      "|green|good|    12|14.386294994851129|         1.0|\n",
      "|green| bad|    16|14.386294994851129|         0.0|\n",
      "|  red|good|    35|14.386294994851129|         1.0|\n",
      "|  red| bad|     1| 38.97187133755819|         0.0|\n",
      "|  red| bad|     2|14.386294994851129|         0.0|\n",
      "|  red| bad|    16|14.386294994851129|         0.0|\n",
      "|  red|good|    45| 38.97187133755819|         1.0|\n",
      "|green|good|     1|14.386294994851129|         1.0|\n",
      "| blue| bad|     8|14.386294994851129|         0.0|\n",
      "| blue| bad|    12|14.386294994851129|         0.0|\n",
      "|green|good|    15| 38.97187133755819|         1.0|\n",
      "|green|good|    12|14.386294994851129|         1.0|\n",
      "|green| bad|    16|14.386294994851129|         0.0|\n",
      "|  red|good|    35|14.386294994851129|         1.0|\n",
      "|  red| bad|     1| 38.97187133755819|         0.0|\n",
      "|  red| bad|     2|14.386294994851129|         0.0|\n",
      "+-----+----+------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Let us create an object of the class StringIndexer\n",
    "lblindexer=StringIndexer().setInputCol(\"lab\").setOutputCol(\"LabelIndexed\")\n",
    "# Let us transform\n",
    "idxRes=lblindexer.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IndexToString\n",
    "\n",
    "Sometimes we come accross situations where it is necessary to convert the indexed values back to text. To do this the Spark ML provides a class IndextoString. To demonstrate the \"IndextoString\" let us use the \"LabelIndexed\" column of  \"idxRes\" dataframe which was created in the previous code snippet.\n",
    "\n",
    "The LabelIndexed column consists of 1.0 --> good and 0.0 --> bad. Nw let us try and reverse this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+------------+------------+\n",
      "|color| lab|value1|            value2|LabelIndexed|ReverseIndex|\n",
      "+-----+----+------+------------------+------------+------------+\n",
      "|green|good|     1|14.386294994851129|         1.0|        good|\n",
      "| blue| bad|     8|14.386294994851129|         0.0|         bad|\n",
      "| blue| bad|    12|14.386294994851129|         0.0|         bad|\n",
      "|green|good|    15| 38.97187133755819|         1.0|        good|\n",
      "|green|good|    12|14.386294994851129|         1.0|        good|\n",
      "|green| bad|    16|14.386294994851129|         0.0|         bad|\n",
      "|  red|good|    35|14.386294994851129|         1.0|        good|\n",
      "|  red| bad|     1| 38.97187133755819|         0.0|         bad|\n",
      "|  red| bad|     2|14.386294994851129|         0.0|         bad|\n",
      "|  red| bad|    16|14.386294994851129|         0.0|         bad|\n",
      "|  red|good|    45| 38.97187133755819|         1.0|        good|\n",
      "|green|good|     1|14.386294994851129|         1.0|        good|\n",
      "| blue| bad|     8|14.386294994851129|         0.0|         bad|\n",
      "| blue| bad|    12|14.386294994851129|         0.0|         bad|\n",
      "|green|good|    15| 38.97187133755819|         1.0|        good|\n",
      "|green|good|    12|14.386294994851129|         1.0|        good|\n",
      "|green| bad|    16|14.386294994851129|         0.0|         bad|\n",
      "|  red|good|    35|14.386294994851129|         1.0|        good|\n",
      "|  red| bad|     1| 38.97187133755819|         0.0|         bad|\n",
      "|  red| bad|     2|14.386294994851129|         0.0|         bad|\n",
      "+-----+----+------+------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "LabelReverse=IndexToString().setInputCol(\"LabelIndexed\").setOutputCol(\"ReverseIndex\")\n",
    "LabelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing within Vectors\n",
    "\n",
    "Spark offer yet another class named \"VectorIndexer\". The \"VectorIndexer\" identifies the categorical variables with a set of features which is already been vectorized and converts it into a categorical feature with zero based category indices.\n",
    "\n",
    "For the purpose of illustration let us first create a new DataFrame with features in the form of Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|     features|labels|\n",
      "+-------------+------+\n",
      "|[1.0,2.0,3.0]|     1|\n",
      "|[2.0,5.0,6.0]|     2|\n",
      "|[1.0,8.0,9.0]|     3|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "dataln=spark.createDataFrame([(Vectors.dense(1,2,3),1),(Vectors.dense(2,5,6),2),(Vectors.dense(1,8,9),3)]).toDF(\"features\",\"labels\")\n",
    "dataln.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-------------+\n",
      "|     features|labels|      indexed|\n",
      "+-------------+------+-------------+\n",
      "|[1.0,2.0,3.0]|     1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|     2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|     3|[0.0,8.0,9.0]|\n",
      "+-------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "VecInd=VectorIndexer().setInputCol(\"features\").setMaxCategories(2).setOutputCol(\"indexed\")\n",
    "VecInd.fit(dataln).transform(dataln).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot endcoding\n",
    "\n",
    "One hot encoder is the most common type of transformation performed during pre-processing. Let us look at an example straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red|good|    45| 38.97187133755819|\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------+-------------+\n",
      "|color| lab|value1|            value2|StrIndexed|   oheIndexed|\n",
      "+-----+----+------+------------------+----------+-------------+\n",
      "|green|good|     1|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "| blue| bad|     8|14.386294994851129|       2.0|    (2,[],[])|\n",
      "| blue| bad|    12|14.386294994851129|       2.0|    (2,[],[])|\n",
      "|green|good|    15| 38.97187133755819|       1.0|(2,[1],[1.0])|\n",
      "|green|good|    12|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "|green| bad|    16|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "|  red|good|    35|14.386294994851129|       0.0|(2,[0],[1.0])|\n",
      "|  red| bad|     1| 38.97187133755819|       0.0|(2,[0],[1.0])|\n",
      "|  red| bad|     2|14.386294994851129|       0.0|(2,[0],[1.0])|\n",
      "|  red| bad|    16|14.386294994851129|       0.0|(2,[0],[1.0])|\n",
      "|  red|good|    45| 38.97187133755819|       0.0|(2,[0],[1.0])|\n",
      "|green|good|     1|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "| blue| bad|     8|14.386294994851129|       2.0|    (2,[],[])|\n",
      "| blue| bad|    12|14.386294994851129|       2.0|    (2,[],[])|\n",
      "|green|good|    15| 38.97187133755819|       1.0|(2,[1],[1.0])|\n",
      "|green|good|    12|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "|green| bad|    16|14.386294994851129|       1.0|(2,[1],[1.0])|\n",
      "|  red|good|    35|14.386294994851129|       0.0|(2,[0],[1.0])|\n",
      "|  red| bad|     1| 38.97187133755819|       0.0|(2,[0],[1.0])|\n",
      "|  red| bad|     2|14.386294994851129|       0.0|(2,[0],[1.0])|\n",
      "+-----+----+------+------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us encode the \"color\" feature in the \"simpleDF\"\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "SI=StringIndexer().setInputCol('color').setOutputCol('StrIndexed')\n",
    "ColorIdx=SI.fit(simpleDF).transform(simpleDF)\n",
    "ohe=OneHotEncoder().setInputCol('StrIndexed').setOutputCol(\"oheIndexed\")\n",
    "ohe.transform(ColorIdx).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this humble blog I have tried to cover some basic and widely used data preprocessing transformations offered by Spark ML. I have demonstrated each of them with an illustration. However, there is a plethora of Spark tools for to aid the feature engineering task some of these I will try to cover in my next blog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "Chambers, B., & Zaharia, M., 2018. Spark: The definitive guide. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
